{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  MicroGrad demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from engine import Value\n",
    "from nn import Neuron, Layer, MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1337)\n",
    "random.seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x145e194e72f0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make up a dataset\n",
    "\n",
    "from sklearn.datasets import make_moons, make_blobs\n",
    "X, y = make_moons(n_samples=100, noise=0.1)\n",
    "\n",
    "y = y*2 - 1 # make y be -1 or 1\n",
    "# visualize in 2D\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.scatter(X[:,0], X[:,1], c=y, s=20, cmap='jet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP of [Layer of [ReLUNeuron(2), ReLUNeuron(2), ReLUNeuron(2), ReLUNeuron(2), ReLUNeuron(2), ReLUNeuron(2), ReLUNeuron(2), ReLUNeuron(2), ReLUNeuron(2), ReLUNeuron(2), ReLUNeuron(2), ReLUNeuron(2), ReLUNeuron(2), ReLUNeuron(2), ReLUNeuron(2), ReLUNeuron(2)], Layer of [ReLUNeuron(16), ReLUNeuron(16), ReLUNeuron(16), ReLUNeuron(16), ReLUNeuron(16), ReLUNeuron(16), ReLUNeuron(16), ReLUNeuron(16), ReLUNeuron(16), ReLUNeuron(16), ReLUNeuron(16), ReLUNeuron(16), ReLUNeuron(16), ReLUNeuron(16), ReLUNeuron(16), ReLUNeuron(16)], Layer of [LinearNeuron(16)]]\n",
      "number of parameters 337\n"
     ]
    }
   ],
   "source": [
    "# initialize a model \n",
    "model = MLP(2, [16, 16, 1]) # 2-layer neural network\n",
    "print(model)\n",
    "print(\"number of parameters\", len(model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value(data=0.8958441028683222, grad=0) 0.5\n"
     ]
    }
   ],
   "source": [
    "# loss function\n",
    "def loss(batch_size=None):\n",
    "    \n",
    "    # inline DataLoader :)\n",
    "    if batch_size is None:\n",
    "        Xb, yb = X, y\n",
    "    else:\n",
    "        ri = np.random.permutation(X.shape[0])[:batch_size]\n",
    "        Xb, yb = X[ri], y[ri]\n",
    "    inputs = [list(map(Value, xrow)) for xrow in Xb]\n",
    "    \n",
    "    # forward the model to get scores\n",
    "    scores = list(map(model, inputs))\n",
    "    \n",
    "    # svm \"max-margin\" loss\n",
    "    losses = [(1 + -yi*scorei).relu() for yi, scorei in zip(yb, scores)]\n",
    "    data_loss = sum(losses) * (1.0 / len(losses))\n",
    "    # L2 regularization\n",
    "    alpha = 1e-4\n",
    "    reg_loss = alpha * sum((p*p for p in model.parameters()))\n",
    "    total_loss = data_loss + reg_loss\n",
    "    \n",
    "    # also get accuracy\n",
    "    accuracy = [(yi > 0) == (scorei.data > 0) for yi, scorei in zip(yb, scores)]\n",
    "    return total_loss, sum(accuracy) / len(accuracy)\n",
    "\n",
    "total_loss, acc = loss()\n",
    "print(total_loss, acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 loss 0.8958441028683222, accuracy 50.0%\n",
      "step 1 loss 1.723590533697202, accuracy 81.0%\n",
      "step 2 loss 0.7429006313851131, accuracy 77.0%\n",
      "step 3 loss 0.7705641260584202, accuracy 82.0%\n",
      "step 4 loss 0.3692793385976538, accuracy 84.0%\n",
      "step 5 loss 0.313545481918522, accuracy 86.0%\n",
      "step 6 loss 0.2814234349772435, accuracy 89.0%\n",
      "step 7 loss 0.2688873331398391, accuracy 91.0%\n",
      "step 8 loss 0.2567147286057417, accuracy 91.0%\n",
      "step 9 loss 0.27048625516379227, accuracy 91.0%\n",
      "step 10 loss 0.24507023853658047, accuracy 91.0%\n",
      "step 11 loss 0.25099055297915035, accuracy 92.0%\n",
      "step 12 loss 0.21560951851922946, accuracy 91.0%\n",
      "step 13 loss 0.23090378446402732, accuracy 93.0%\n",
      "step 14 loss 0.2015215122789945, accuracy 92.0%\n",
      "step 15 loss 0.22574506279282222, accuracy 93.0%\n",
      "step 16 loss 0.1944798759620412, accuracy 92.0%\n",
      "step 17 loss 0.21089496199246358, accuracy 93.0%\n",
      "step 18 loss 0.15983077356303613, accuracy 94.0%\n",
      "step 19 loss 0.1845374874688392, accuracy 93.0%\n",
      "step 20 loss 0.18977522856087642, accuracy 91.0%\n",
      "step 21 loss 0.19072704042579644, accuracy 93.0%\n",
      "step 22 loss 0.11733695088756486, accuracy 97.0%\n",
      "step 23 loss 0.12173524408232457, accuracy 95.0%\n",
      "step 24 loss 0.12615712612770458, accuracy 95.0%\n",
      "step 25 loss 0.16049097780801672, accuracy 95.0%\n",
      "step 26 loss 0.1874719770524581, accuracy 92.0%\n",
      "step 27 loss 0.16741837891059408, accuracy 95.0%\n",
      "step 28 loss 0.09586583491455399, accuracy 97.0%\n",
      "step 29 loss 0.08778783707420917, accuracy 96.0%\n",
      "step 30 loss 0.11731297569011853, accuracy 95.0%\n",
      "step 31 loss 0.09340146460619841, accuracy 97.0%\n",
      "step 32 loss 0.12454454903103455, accuracy 95.0%\n",
      "step 33 loss 0.07984002652777274, accuracy 97.0%\n",
      "step 34 loss 0.07727519232921672, accuracy 97.0%\n",
      "step 35 loss 0.07661250143094481, accuracy 98.0%\n",
      "step 36 loss 0.10610492379198364, accuracy 96.0%\n",
      "step 37 loss 0.09062808429265987, accuracy 99.0%\n",
      "step 38 loss 0.10671887043036928, accuracy 95.0%\n",
      "step 39 loss 0.052256599219758476, accuracy 98.0%\n",
      "step 40 loss 0.06016009895234476, accuracy 100.0%\n",
      "step 41 loss 0.0859672453333394, accuracy 96.0%\n",
      "step 42 loss 0.051121079431796015, accuracy 99.0%\n",
      "step 43 loss 0.05240142401642829, accuracy 97.0%\n",
      "step 44 loss 0.04530684179001578, accuracy 100.0%\n",
      "step 45 loss 0.07211073370655093, accuracy 97.0%\n",
      "step 46 loss 0.033342386513102354, accuracy 99.0%\n",
      "step 47 loss 0.03143222795751129, accuracy 100.0%\n",
      "step 48 loss 0.036585367471115016, accuracy 99.0%\n",
      "step 49 loss 0.048291393823903274, accuracy 99.0%\n",
      "step 50 loss 0.09875114765619616, accuracy 96.0%\n",
      "step 51 loss 0.05449063965875453, accuracy 99.0%\n",
      "step 52 loss 0.033926794357083206, accuracy 100.0%\n",
      "step 53 loss 0.05261517263568437, accuracy 97.0%\n",
      "step 54 loss 0.03250295251424918, accuracy 99.0%\n",
      "step 55 loss 0.028883273872078324, accuracy 100.0%\n",
      "step 56 loss 0.04139151104027232, accuracy 98.0%\n",
      "step 57 loss 0.01898740742612855, accuracy 100.0%\n",
      "step 58 loss 0.025238335238837353, accuracy 100.0%\n",
      "step 59 loss 0.020796565213419008, accuracy 100.0%\n",
      "step 60 loss 0.03259711157810224, accuracy 99.0%\n",
      "step 61 loss 0.017863351693480377, accuracy 100.0%\n",
      "step 62 loss 0.023008717832211673, accuracy 100.0%\n",
      "step 63 loss 0.022079325463581594, accuracy 100.0%\n",
      "step 64 loss 0.029432917853529608, accuracy 99.0%\n",
      "step 65 loss 0.016251514644092028, accuracy 100.0%\n",
      "step 66 loss 0.02846853448326438, accuracy 99.0%\n",
      "step 67 loss 0.013994365546208717, accuracy 100.0%\n",
      "step 68 loss 0.015552344843651564, accuracy 100.0%\n",
      "step 69 loss 0.033891199461601594, accuracy 99.0%\n",
      "step 70 loss 0.014229870065926903, accuracy 100.0%\n",
      "step 71 loss 0.013255281583285485, accuracy 100.0%\n",
      "step 72 loss 0.01230027759002204, accuracy 100.0%\n",
      "step 73 loss 0.01267605249835614, accuracy 100.0%\n",
      "step 74 loss 0.02059381195595468, accuracy 100.0%\n",
      "step 75 loss 0.011845398205364403, accuracy 100.0%\n",
      "step 76 loss 0.016012697472883315, accuracy 100.0%\n",
      "step 77 loss 0.025458360239222027, accuracy 100.0%\n",
      "step 78 loss 0.014382930289661856, accuracy 100.0%\n",
      "step 79 loss 0.011698962425817942, accuracy 100.0%\n",
      "step 80 loss 0.01231850080051585, accuracy 100.0%\n",
      "step 81 loss 0.014121117031464186, accuracy 100.0%\n",
      "step 82 loss 0.011664591962446177, accuracy 100.0%\n",
      "step 83 loss 0.011589314549188757, accuracy 100.0%\n",
      "step 84 loss 0.010990299347735226, accuracy 100.0%\n",
      "step 85 loss 0.01098922672069161, accuracy 100.0%\n",
      "step 86 loss 0.01098819375765507, accuracy 100.0%\n",
      "step 87 loss 0.010987200447388705, accuracy 100.0%\n",
      "step 88 loss 0.010986246779084923, accuracy 100.0%\n",
      "step 89 loss 0.010985332742365274, accuracy 100.0%\n",
      "step 90 loss 0.010984458327280174, accuracy 100.0%\n",
      "step 91 loss 0.01098362352430886, accuracy 100.0%\n",
      "step 92 loss 0.010982828324359071, accuracy 100.0%\n",
      "step 93 loss 0.010982072718767001, accuracy 100.0%\n",
      "step 94 loss 0.010981356699297042, accuracy 100.0%\n",
      "step 95 loss 0.010980680258141723, accuracy 100.0%\n",
      "step 96 loss 0.010980043387921508, accuracy 100.0%\n",
      "step 97 loss 0.010979446081684673, accuracy 100.0%\n",
      "step 98 loss 0.010978888332907229, accuracy 100.0%\n",
      "step 99 loss 0.010978370135492715, accuracy 100.0%\n"
     ]
    }
   ],
   "source": [
    "# optimization\n",
    "for k in range(100):\n",
    "    \n",
    "    # forward\n",
    "    total_loss, acc = loss()\n",
    "    \n",
    "    # backward\n",
    "    model.zero_grad()\n",
    "    total_loss.backward()\n",
    "    \n",
    "    # update (sgd)\n",
    "    learning_rate = 1.0 - 0.9*k/100\n",
    "    for p in model.parameters():\n",
    "        p.data -= learning_rate * p.grad\n",
    "    \n",
    "    if k % 1 == 0:\n",
    "        print(f\"step {k} loss {total_loss.data}, accuracy {acc*100}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1.548639298268643, 1.951360701731357)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# visualize decision boundary\n",
    "\n",
    "h = 0.25\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "Xmesh = np.c_[xx.ravel(), yy.ravel()]\n",
    "inputs = [list(map(Value, xrow)) for xrow in Xmesh]\n",
    "scores = list(map(model, inputs))\n",
    "Z = np.array([s.data > 0 for s in scores])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral, alpha=0.8)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral)\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
